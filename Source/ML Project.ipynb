{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import scipy\n",
    "import copy\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = pd.read_csv(\"multilabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>@jbromley29 Science is only good when it says ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>none</td>\n",
       "      <td>Every day I think maybe it will be the day I f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>*sigh* oh Colin üòç #MKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>none</td>\n",
       "      <td>Just read the phrase \"feminist agenda\" and sno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>It's insane they keep bringing people back. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>none</td>\n",
       "      <td>Definitely read, forward, read again: http://t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>none</td>\n",
       "      <td>@berkeley_eagle #MKR  this shit show has more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>none</td>\n",
       "      <td>\"Sexism can work both ways\" = I can stop for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>none</td>\n",
       "      <td>@twothug4u educate yourself on oppression. htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>none</td>\n",
       "      <td>Wishing a horde of rabid bats on @Trizzzy13 an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>none</td>\n",
       "      <td>@LeadfootedLion your tweet said \"call me sexis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>none</td>\n",
       "      <td>Congratulations, Illinois, on becoming the 15t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>none</td>\n",
       "      <td>@punk_manners why choose when you can hate bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>none</td>\n",
       "      <td>Halloween is a busy day for sexist assholes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>none</td>\n",
       "      <td>The best part about Twitter is seeing the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>none</td>\n",
       "      <td>@jasonverlanderr my mistake! Sounded like it c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>none</td>\n",
       "      <td>@JonathanGrider7 Also, @YesYoureRacist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>none</td>\n",
       "      <td>I think the worst is when women shame other wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>none</td>\n",
       "      <td>I don't think misogynists have figured out tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>none</td>\n",
       "      <td>@LianaBrooks @docfreeride But then all that sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>none</td>\n",
       "      <td>I'm sure I've linked before, but it deserves e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>none</td>\n",
       "      <td>@LeviathanPride Men like you should have their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>none</td>\n",
       "      <td>The only thing women should do is not give a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>none</td>\n",
       "      <td>The visceral hate that men have 4 women is why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>none</td>\n",
       "      <td>@james19XX You should examine your personal re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>none</td>\n",
       "      <td>@jamiecowan92 I wish a bell would ring and con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>none</td>\n",
       "      <td>Has anyone fallen into the irony vortex @MadJo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>none</td>\n",
       "      <td>@MikeDePasquale2 wins the \"Wow, I Don't Even K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>none</td>\n",
       "      <td>RT @jamiecowan92: @RayDouglas777 @YesYoureSexi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>none</td>\n",
       "      <td>And today one of my students used the word \"ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4470</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== Links ==NEWLINE_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>`NEWLINE_TOKEN==Combat Range==NEWLINE_TOKENRem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>2005 (UTC)NEWLINE_TOKENNEWLINE_TOKEN:If you me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN==Wikipedia:Votes fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4474</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== Batteries ==NEWLI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== Here's what you s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4476</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKEN Survey says, chanting fox is idi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKEN 1st off, Im not banned, 2nd of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== Informational sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENDAMN ur an ugly fukka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN==Cerebral hemorrage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>`NEWLINE_TOKEN:Well it's in the logo, and is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>I think it is rather obvious what to do sir. Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>Well Ray, I actually understand copyright bett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>This is the shared ip of a swiss school. There...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN::::I considered ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>, so we have difficulties to handle √èÀÜ's excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>I don't object to characterizing International...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>`By default of being queen she is ``Her Majest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN::yes, you made tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN:Heh, I probably wen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>May 2005 (UTC)NEWLINE_TOKENNEWLINE_TOKEN:Eagle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENI supose we are wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4493</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKEN:I just took a lok at Fidonet tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>I think that is actually useful information th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>Latest ESA info suggest launch in 2013. I am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENYes, of course - but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>` 2005 (UTC)NEWLINE_TOKENNEWLINE_TOKEN:As as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>`look, Raj, I apologize again for the ``simply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>PersonalAttack</td>\n",
       "      <td>NEWLINE_TOKEN::If you check Skyring's user con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               label                                            comment\n",
       "0               none  @jbromley29 Science is only good when it says ...\n",
       "1               none  Every day I think maybe it will be the day I f...\n",
       "2               none                             *sigh* oh Colin üòç #MKR\n",
       "3               none  Just read the phrase \"feminist agenda\" and sno...\n",
       "4               none  It's insane they keep bringing people back. Wh...\n",
       "5               none  Definitely read, forward, read again: http://t...\n",
       "6               none  @berkeley_eagle #MKR  this shit show has more ...\n",
       "7               none  \"Sexism can work both ways\" = I can stop for t...\n",
       "8               none  @twothug4u educate yourself on oppression. htt...\n",
       "9               none  Wishing a horde of rabid bats on @Trizzzy13 an...\n",
       "10              none  @LeadfootedLion your tweet said \"call me sexis...\n",
       "11              none  Congratulations, Illinois, on becoming the 15t...\n",
       "12              none  @punk_manners why choose when you can hate bot...\n",
       "13              none       Halloween is a busy day for sexist assholes.\n",
       "14              none  The best part about Twitter is seeing the same...\n",
       "15              none  @jasonverlanderr my mistake! Sounded like it c...\n",
       "16              none             @JonathanGrider7 Also, @YesYoureRacist\n",
       "17              none  I think the worst is when women shame other wo...\n",
       "18              none  I don't think misogynists have figured out tha...\n",
       "19              none  @LianaBrooks @docfreeride But then all that sw...\n",
       "20              none  I'm sure I've linked before, but it deserves e...\n",
       "21              none  @LeviathanPride Men like you should have their...\n",
       "22              none  The only thing women should do is not give a f...\n",
       "23              none  The visceral hate that men have 4 women is why...\n",
       "24              none  @james19XX You should examine your personal re...\n",
       "25              none  @jamiecowan92 I wish a bell would ring and con...\n",
       "26              none  Has anyone fallen into the irony vortex @MadJo...\n",
       "27              none  @MikeDePasquale2 wins the \"Wow, I Don't Even K...\n",
       "28              none  RT @jamiecowan92: @RayDouglas777 @YesYoureSexi...\n",
       "29              none  And today one of my students used the word \"ra...\n",
       "...              ...                                                ...\n",
       "4470  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKEN== Links ==NEWLINE_T...\n",
       "4471  PersonalAttack  `NEWLINE_TOKEN==Combat Range==NEWLINE_TOKENRem...\n",
       "4472  PersonalAttack  2005 (UTC)NEWLINE_TOKENNEWLINE_TOKEN:If you me...\n",
       "4473  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKEN==Wikipedia:Votes fo...\n",
       "4474  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKEN== Batteries ==NEWLI...\n",
       "4475  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKEN== Here's what you s...\n",
       "4476  PersonalAttack  NEWLINE_TOKEN Survey says, chanting fox is idi...\n",
       "4477  PersonalAttack  NEWLINE_TOKEN 1st off, Im not banned, 2nd of, ...\n",
       "4478  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKEN== Informational sit...\n",
       "4479  PersonalAttack    NEWLINE_TOKENNEWLINE_TOKENDAMN ur an ugly fukka\n",
       "4480  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKEN==Cerebral hemorrage...\n",
       "4481  PersonalAttack  `NEWLINE_TOKEN:Well it's in the logo, and is a...\n",
       "4482  PersonalAttack  I think it is rather obvious what to do sir. Y...\n",
       "4483  PersonalAttack  Well Ray, I actually understand copyright bett...\n",
       "4484  PersonalAttack  This is the shared ip of a swiss school. There...\n",
       "4485  PersonalAttack  `NEWLINE_TOKENNEWLINE_TOKEN::::I considered ch...\n",
       "4486  PersonalAttack  , so we have difficulties to handle √èÀÜ's excha...\n",
       "4487  PersonalAttack  I don't object to characterizing International...\n",
       "4488  PersonalAttack  `By default of being queen she is ``Her Majest...\n",
       "4489  PersonalAttack  `NEWLINE_TOKENNEWLINE_TOKEN::yes, you made tha...\n",
       "4490  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKEN:Heh, I probably wen...\n",
       "4491  PersonalAttack  May 2005 (UTC)NEWLINE_TOKENNEWLINE_TOKEN:Eagle...\n",
       "4492  PersonalAttack  `NEWLINE_TOKENNEWLINE_TOKENI supose we are wri...\n",
       "4493  PersonalAttack  NEWLINE_TOKEN:I just took a lok at Fidonet tod...\n",
       "4494  PersonalAttack  I think that is actually useful information th...\n",
       "4495  PersonalAttack  Latest ESA info suggest launch in 2013. I am l...\n",
       "4496  PersonalAttack  NEWLINE_TOKENNEWLINE_TOKENYes, of course - but...\n",
       "4497  PersonalAttack  ` 2005 (UTC)NEWLINE_TOKENNEWLINE_TOKEN:As as a...\n",
       "4498  PersonalAttack  `look, Raj, I apologize again for the ``simply...\n",
       "4499  PersonalAttack  NEWLINE_TOKEN::If you check Skyring's user con...\n",
       "\n",
       "[4500 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamscott/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "features = np.array(master_data.ix[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamscott/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(master_data.ix[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none', 'none', 'none', ..., 'PersonalAttack', 'PersonalAttack',\n",
       "       'PersonalAttack'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# import re\n",
    "# text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = [[\"I removed ftp://github.com/ZeerakW/hatespeech/blob/master/NAACL_SRW_2016.csv the following:NEWLINE_TOKENNEWLINE_TOKENAll names of early Polish rulers are ficticious and therefore this index naming Oda von Haldensleben and her husband Dagome records for the first time rulers of the Polanen tribe. Therefore it is indicated as being the first document of the later developing land named Poland.NEWLINE_TOKENNEWLINE_TOKENThis is quite a comment. All names are fictitious? It deserves at least some backing.\"],\n",
    "#                  [\"Therefore it is indicated as being the first document of the later developing land named Poland.NEWLINE_TOKENNEWLINE_TOKENThis is quite a comment. All names are fictitious? It deserves at least some backing.\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = re.sub(r'^httasdfps?:\\/\\/.*[\\r\\n]*', '', test, flags=re.MULTILINE)\n",
    "test = features\n",
    "p_data = []\n",
    "\n",
    "for k in test:\n",
    "#     print(k)\n",
    "    p_data.append([re.sub(r\"(http|ftp|https)\\S+\", \"\", str(k))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(arr):\n",
    "    return np.char.lower(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(arr):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    filteredWords = []\n",
    "    for a in arr:\n",
    "        words = word_tokenize(str(a[0]))\n",
    "        wordsFiltered = \"\"\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered = wordsFiltered + \" \" + w\n",
    "        filteredWords.append([wordsFiltered[:len(wordsFiltered)-1]])\n",
    "    a = np.asarray(filteredWords)\n",
    "    return np.char.strip(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSymbols(arr):\n",
    "    symbols = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    for i in range(len(symbols)):\n",
    "        arr = np.char.replace(arr, symbols[i], '')\n",
    "    \n",
    "    return np.char.replace(arr, \"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(word):\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    filteredWords = []\n",
    "    for a in word:\n",
    "        words = word_tokenize(str(a[0]))\n",
    "        wordsFiltered = \"\"\n",
    "        for w in words:\n",
    "            wordsFiltered = wordsFiltered + \" \" + porter_stemmer.stem(w)\n",
    "        filteredWords.append([wordsFiltered[:len(wordsFiltered)-1]])\n",
    "    a = np.asarray(filteredWords)\n",
    "    return np.char.strip(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeImplicitNoise(arr):\n",
    "    return np.char.replace (arr, 'NEWLINE_TOKEN', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processClean(data):\n",
    "     \n",
    "    a = removeImplicitNoise(data)\n",
    "\n",
    "    a = lowercase(a)\n",
    "\n",
    "    a = removeStopWords(a)\n",
    "\n",
    "    a = removeSymbols(a)\n",
    "    \n",
    "    a = stemming(a)\n",
    "\n",
    "    return a\n",
    "\n",
    "a = processClean(test)\n",
    "\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_l = []\n",
    "\n",
    "for i in a:\n",
    "    list_l.append(str(i[0]))\n",
    "# print(list_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in a:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/util.py\u001b[0m in \u001b[0;36mngrams\u001b[0;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-787a65bcdeda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrammed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgrammed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-787a65bcdeda>\u001b[0m in \u001b[0;36mmake_grams\u001b[0;34m(data, n)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0msixgrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mgrams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msixgrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "def make_grams(data, n = 3):\n",
    "    \n",
    "    grammed_data = []\n",
    "    \n",
    "    for i in data:\n",
    "#         print(\"\\nT:\",i)\n",
    "\n",
    "        k = copy.deepcopy(i)\n",
    "\n",
    "        for r in range(2,n+1):\n",
    "            sixgrams = ngrams(i.split(), r)\n",
    "\n",
    "            for grams in sixgrams:\n",
    "                g = \"\"\n",
    "                for p in grams:\n",
    "                    g = g+p\n",
    "\n",
    "                k = k+\" \"+g\n",
    "#         print(\"\\nK:\",k)\n",
    "        grammed_data.append(k)\n",
    "    \n",
    "    return grammed_data\n",
    "\n",
    "grammed_data = make_grams(list_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(grammed_data)\n",
    "vector = vectorizer.transform(grammed_data)\n",
    "\n",
    "feature_vector = vector.toarray()\n",
    "print(feature_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_vector, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = MultinomialNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = gnb.score(X_test, y_test)\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(grammed_data)\n",
    "vector = vectorizer.transform(grammed_data)\n",
    "\n",
    "feature_vector = vector.toarray()\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
